# LiteLLM Proxy Configuration for marketing.tvoje.info
#
# Usage:
#   1. Copy .env.example to .env and add your API keys
#   2. Start proxy: litellm --config proxy_config.yaml
#   3. Test: curl http://localhost:4000/v1/chat/completions -H "Content-Type: application/json" -d '{"model": "groq/llama-3.3-70b-versatile", "messages": [{"role": "user", "content": "Hello"}]}'
#
# Rate Limits (Groq Developer Plan - Free Tier):
#   - llama-3.1-8b-instant:  30 RPM, 14.4K RPD, 6K TPM, 500K TPD (fast, cheap)
#   - llama-3.3-70b-versatile: 30 RPM, 1K RPD, 12K TPM, 100K TPD (quality)
#   - groq/compound: 30 RPM, 250 RPD, 70K TPM (agentic AI)
#
# Pricing (per 1M tokens):
#   - llama-3.1-8b-instant: $0.05 input / $0.08 output
#   - llama-3.3-70b-versatile: $0.59 input / $0.79 output
#
# Last Verified: 2026-02-13

model_list:
  # ========================================
  # Groq Models (Primary - Fast & Free Tier)
  # ========================================

  # llama-3.3-70b-versatile - Quality model for complex tasks
  # Rate Limits: 30 RPM, 1K RPD, 12K TPM, 100K TPD
  # Pricing: $0.59/$0.79 per 1M tokens
  # Context: 131K tokens | Speed: 280 T/s
  - model_name: groq/llama-3.3-70b-versatile
    litellm_params:
      model: groq/llama-3.3-70b-versatile
      api_key: os.environ/GROQ_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: false

  - model_name: groq-fast-agent
    litellm_params:
      model: groq/llama-3.3-70b-versatile
      api_key: os.environ/GROQ_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true

  # llama-3.1-8b-instant - Fast, cheap model for simple tasks
  # Rate Limits: 30 RPM, 14.4K RPD, 6K TPM, 500K TPD
  # Pricing: $0.05/$0.08 per 1M tokens
  # Context: 131K tokens | Speed: 560 T/s
  - model_name: groq/llama-3.1-8b-instant
    litellm_params:
      model: groq/llama-3.1-8b-instant
      api_key: os.environ/GROQ_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true

  # llama-3.1-70b-versatile - Alternative quality model
  # Rate Limits: 30 RPM, 1K RPD, 12K TPM, 100K TPD
  - model_name: groq/llama-3.1-70b-versatile
    litellm_params:
      model: groq/llama-3.1-70b-versatile
      api_key: os.environ/GROQ_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true

  # ========================================
  # Groq OpenAI-Compatible Models (New)
  # ========================================

  # openai/gpt-oss-120b - Large OpenAI model on Groq
  # Rate Limits: 30 RPM, 1K RPD, 8K TPM, 200K TPD
  - model_name: openai/gpt-oss-120b
    litellm_params:
      model: openai/gpt-oss-120b
      api_key: os.environ/GROQ_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true

  # openai/gpt-oss-20b - Smaller OpenAI model on Groq
  - model_name: openai/gpt-oss-20b
    litellm_params:
      model: openai/gpt-oss-20b
      api_key: os.environ/GROQ_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true

  # ========================================
  # Groq Agentic AI Models (New)
  # ========================================

  # groq/compound - Agentic AI with built-in tools
  # Rate Limits: 30 RPM, 250 RPD, 70K TPM
  # Use for: Multi-step reasoning, tool use, agentic workflows
  - model_name: groq/compound
    litellm_params:
      model: groq/compound
      api_key: os.environ/GROQ_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true

  # groq/compound-mini - Lightweight agentic AI
  - model_name: groq/compound-mini
    litellm_params:
      model: groq/compound-mini
      api_key: os.environ/GROQ_API_KEY
    model_info:
      mode: chat
      supports_function_calling: true

  # OpenAI models (requires API key)
  - model_name: gpt-4o-mini
    litellm_params:
      model: gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY

  - model_name: gpt-4o
    litellm_params:
      model: gpt-4o
      api_key: os.environ/OPENAI_API_KEY

  # Anthropic models (requires API key)
  - model_name: claude-3-haiku
    litellm_params:
      model: anthropic/claude-3-haiku-20240307
      api_key: os.environ/ANTHROPIC_API_KEY

  - model_name: claude-3-sonnet
    litellm_params:
      model: anthropic/claude-3-sonnet-20240229
      api_key: os.environ/ANTHROPIC_API_KEY

  # Gemini models (requires API key)
  - model_name: gemini-1.5-flash
    litellm_params:
      model: gemini/gemini-1.5-flash
      api_key: os.environ/GOOGLE_API_KEY

litellm_settings:
  drop_params: true
  request_timeout: 120
  num_retries: 3
  telemetry: false

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  ui_access_mode: admin

# ========================================
# Fallback Configuration
# ========================================
# Automatic fallback chain when primary models fail
# Groq -> OpenRouter -> KiloCode (GLM-5)
fallbacks:
  # Primary quality model fallback chain
  - model: groq/llama-3.3-70b-versatile
    fallbacks:
      - groq/llama-3.1-8b-instant
      - gpt-4o-mini

  # Fast model fallback chain
  - model: groq/llama-3.1-8b-instant
    fallbacks:
      - gpt-4o-mini

  # Agentic AI fallback chain
  - model: groq/compound
    fallbacks:
      - groq/llama-3.3-70b-versatile
      - groq/llama-3.1-8b-instant

  # OpenAI models on Groq fallback
  - model: openai/gpt-oss-120b
    fallbacks:
      - groq/llama-3.3-70b-versatile
      - gpt-4o-mini
